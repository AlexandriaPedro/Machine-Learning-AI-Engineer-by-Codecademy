MACHINE LEARNING/AI ENGINEER
PCA Project
In this project, you will classify particles into gamma(signal) or hadrons(background). Given that the features are correlated, you will perform PCA to get a new set of features, and select the features that contain the most information. The data set was generated by a Monte Carlo program, Corsika, described in D. Heck et al., CORSIKA, A Monte Carlo code to simulate extensive air showers, Forschungszentrum Karlsruhe FZKA 6019 (1998).

The first part of the project ‘Observing the Dataset’ is in script_1.py, with its solution in script_1_solution.py. The second part of the project ‘Performing PCA’ is in script_2.py, with its solution in script_2_solution.py. It is highly recommend that you complete the project on your own without checking the solutions, but feel free to take a look if you get stuck or want to check your answers when you’re done!

Tasks
14/14 complete
Mark the tasks as complete by checking them off
Observing the Dataset
1.
Remove any nulls from the dataset.

2.
Extract the numerical columns onto a variable named data_matrix and the classes to a variable named classes

3.
Create a correlation matrix of the data matrix and show it as a heatmap.

4.
Find the eigenvectors and eigenvalues using the NumPy function np.linalg.eig. Here we also order the eigenvalues from greatest to smallest by ordering its indices first, and use these indices to also order the eigenvalues.

5.
Find the proportions of each eigenvalue to the total sum of the eigenvalues. These proportions represent the percentages of information that each eigenvalue’s associated eigenvector contains.

6.
Find the cumulative percentages of the ordered eigenvectors.

Performing PCA
7.
Recall that PCA uses the standardized matrix, which scales for the mean and standard deviation. Calculate the standardized data matrix.

8.
Using the sklearn module PCA, perform PCA by fitting and transforming the standardized data matrix.

9.
Using the properties of the PCA trained object, pca, calculate the eigenvalues from the singular values and extract the eigenvectors.

10.
Using the properties of the pca trained object, extract the variance ratios, which are equivalent to the eigenvalue proportions we calculated earlier.

11.
Perform PCA once again but with 2 principal axes. Make sure to fit and transform the standardized data matrix.

12.
Plot the principal components and have its class as its hue to see if clustering of any kind has occurred.

13.
Once again, we will perform PCA on 2 components, but we will also use the newly transformed PCA features as input to a support vector classifier! Fit the transformed features onto the classifier and generate a score.

14.
Now, fit the classifier with the first two features of the original standardized data matrix and generate a score.

Looking at the two SVG models, the first model with the PCA features and the second model with the original data features, which model has a higher accuracy score?